{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Generate_stock_data as GD\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals, print_function, division, absolute_import\n",
    "from io import open\n",
    "import unicodedata, string, re, random, math, time, shutil, pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "model_path = \"./stock_dual/\"\n",
    "\n",
    "# Network Parameters\n",
    "# encoder parameter\n",
    "n_input_encoder = 81 # n_feature of encoder input\n",
    "n_steps_encoder = 10 # time steps\n",
    "n_hidden_encoder = 128 # size of hidden units\n",
    "\n",
    "# decoder parameter\n",
    "n_input_decoder = 1\n",
    "n_steps_decoder = 9\n",
    "n_hidden_decoder = 128\n",
    "n_classes = 1 # size of the decoder output\n",
    "\n",
    "loss_value = []\n",
    "step_value = []\n",
    "loss_test=[]\n",
    "loss_val = []\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# Get the data from here.\n",
    "Data = GD.Input_data(batch_size, n_steps_encoder, n_steps_decoder, n_hidden_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# class AttnDec(nn.Module):\n",
    "#     def __init__(self, decoder_state_size, encoder_state_size):\n",
    "#         super(AttnDec, self).__init__()\n",
    "#         self.decoder_state_size = decoder_state_size\n",
    "#         self.cell_state_size = decoder_state_size\n",
    "#         self.encoder_state_size = encoder_state_size\n",
    "#         self.Attn_Decoder_V = nn.Parameter(torch.FloatTensor(self.encoder_state_size,1))\n",
    "#         self.Attn_Decoder_W = nn.Linear(self.decoder_state_size + self.cell_state_size, self.encoder_state_size)\n",
    "#         self.Attn_Decoder_U = nn.Linear(self.encoder_state_size, self.encoder_state_size)\n",
    "\n",
    "#     def forward(self, decoder_state, cell_state, encoder_outputs):\n",
    "#         '''\n",
    "#         decoder_state(batch_size, decoder_state_size): d_t\n",
    "#         cell_state(batch_size, cell_state_size): s_t\n",
    "#         encoder_outputs(batch_size, n_steps_encoder, hidden_size): h_1 h_2 ... h_T\n",
    "#         '''\n",
    "#         this_batch_size = encoder_outputs.size(0)\n",
    "#         max_len = encoder_outputs.size(1)\n",
    "#         # change w v u  linears to all variables - prahal\n",
    "#         # no need for the for loop because we can do axis wise multiplication\n",
    "#         # Create variable to store attention energies\n",
    "#         attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x T\n",
    "\n",
    "#         if use_cuda:\n",
    "#             attn_energies = attn_energies.cuda()\n",
    "\n",
    "#         combined_state = torch.cat((decoder_state, cell_state),0)\n",
    "#         # Calculate energy for each encoder output\n",
    "#         for i in range(max_len):\n",
    "#             attn_energies[:, i] = self.score(combined_state, encoder_outputs[:, i, :])\n",
    "        \n",
    "#         return F.softmax(attn_energies) # B x T\n",
    "   \n",
    "#     def score(self, combined_hidden_state, encoder_output):\n",
    "#         '''\n",
    "#         combined_hidden_state: [d_t;s_t]\n",
    "#         encoder_output: h_t\n",
    "#         '''\n",
    "#         #v^T*tanh(W*[d_t;s_t] + U*h_i)\n",
    "#         return F.tanh(self.Attn_Decoder_W(combined_hidden_state) + self.Attn_Decoder_U(encoder_output)).mm(self.Attn_Decoder_V)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module): #Exactly same as AttnDec. It's redundant.\n",
    "    def __init__(self, decoder_state_size, encoder_state_size): #(128, 10)\n",
    "        super(Attn, self).__init__()\n",
    "        self.decoder_state_size = decoder_state_size\n",
    "        self.cell_state_size = decoder_state_size\n",
    "        self.encoder_state_size = encoder_state_size\n",
    "        self.Attn_Decoder_V = nn.Parameter(torch.FloatTensor(self.encoder_state_size,1))\n",
    "        self.Attn_Decoder_W = nn.Linear(self.decoder_state_size + self.cell_state_size, self.encoder_state_size)\n",
    "        self.Attn_Decoder_U = nn.Linear(self.encoder_state_size, self.encoder_state_size)\n",
    "\n",
    "    def forward(self, decoder_state, cell_state, encoder_outputs):\n",
    "        '''\n",
    "        decoder_state(batch_size, decoder_state_size): h_t\n",
    "        cell_state(batch_size, cell_state_size): s_t\n",
    "        encoder_outputs(batch_size, n_input_encoder, n_steps_encoder): X\n",
    "        '''\n",
    "        this_batch_size = encoder_outputs.size(0)\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        # change w v u  linears to all variables - prahal\n",
    "        # no need for the for loop because we can do axis wise multiplication\n",
    "        # Create variable to store attention energies\n",
    "        # combined_state ?? double check\n",
    "        # init inital cell state\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x N\n",
    "\n",
    "        if use_cuda:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "        \n",
    "        combined_state = torch.cat((decoder_state, cell_state),1)\n",
    "        # Calculate energy for each encoder output\n",
    "        for i in range(max_len):\n",
    "            attn_energies[:, i] = self.score(combined_state, encoder_outputs[:, i, :])\n",
    "        \n",
    "        return F.softmax(attn_energies) # B x N\n",
    "   \n",
    "    def score(self, combined_hidden_state, encoder_output):\n",
    "        '''\n",
    "        combined_hidden_state: [d_t;s_t]\n",
    "        encoder_output: h_t\n",
    "        '''\n",
    "#         pdb.set_trace()\n",
    "        #v^T*tanh(W*[d_t;s_t] + U*h_i)\n",
    "        return F.tanh(self.Attn_Decoder_W(combined_hidden_state) + self.Attn_Decoder_U(encoder_output)).mm(self.Attn_Decoder_V)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder\n",
    "[Reference](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size = n_input_decoder, hidden_size = n_hidden_decoder, output_size = n_classes, \n",
    "                 n_layers = 1, dropout_p = 0, encoder_hidden_size = n_hidden_encoder):\n",
    "        '''\n",
    "        input_size: size of input y_i=1\n",
    "        hidden_size: size of d_t=128\n",
    "        output_size: n_classes=1\n",
    "        n_layers: number of hidden layers=1\n",
    "        encoder_hidden_size: size of h_t=128\n",
    "        '''\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.decoder_input_size = input_size\n",
    "        self.decoder_hidden_size = hidden_size #size of d_t\n",
    "        self.decoder_output_size = output_size #size of y_t\n",
    "        self.n_layers = n_layers\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        #self.dropout = nn.Dropout(dropout_p)\n",
    "        self.linear_layer_input = nn.Linear(self.decoder_input_size + self.encoder_hidden_size, self.decoder_input_size)\n",
    "        self.linear_layer_output = nn.Linear(self.decoder_hidden_size + self.encoder_hidden_size, self.decoder_hidden_size)\n",
    "        self.decoder_cell = nn.LSTMCell(self.decoder_input_size, self.decoder_hidden_size)\n",
    "        self.attn = Attn(self.decoder_hidden_size, self.encoder_hidden_size)\n",
    "        #print (len(list(self.attn.parameters())))\n",
    "        \n",
    "    '''encoder_output:Not in use right now'''\n",
    "    def forward(self, decoder_input, decoder_hidden, decoder_cell_state, encoder_outputs, context_vector):\n",
    "        '''\n",
    "        decoder_input(batch_size, input_size): y_i (batch_size x 1)\n",
    "        decoder_hidden(batch_size, hidden_size): initial hidden state\n",
    "        encoder_outputs(batch_size, encoder_size): h_1 ... h_T\n",
    "                                    encoder_size-currently: (10 x 128)\n",
    "        context_vector(batch_size, n_hidden_encoder): c_t\n",
    "        '''\n",
    "        #tempTest = Variable(torch.zeros(batch_size, self.decoder_input_size + self.encoder_hidden_size))\n",
    "        concat_decoder_input = self.linear_layer_input(torch.cat((decoder_input, context_vector),1)) # B x 1\n",
    "        \n",
    "        #replace decoder_hidden by self.decoder_hidden_state?\n",
    "        decoder_hidden, decoder_cell_state = self.decoder_cell(concat_decoder_input, (decoder_hidden, decoder_cell_state))\n",
    "        \n",
    "        attn_weights = self.attn(decoder_hidden, decoder_cell_state, encoder_outputs).unsqueeze(2).expand_as(encoder_outputs) # B x T\n",
    "        context_vector = torch.sum(attn_weights * encoder_outputs, 1) # B x encoder_hidden_size\n",
    "        decoder_output = self.linear_layer_output(torch.cat((decoder_hidden, context_vector),1)) # B x 1\n",
    "        return decoder_hidden, decoder_cell_state, decoder_output, context_vector\n",
    "    \n",
    "    def initHidden(self, encoder_hidden, encoder_cell_state):\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell_state = encoder_cell_state\n",
    "        if use_cuda:\n",
    "            return (decoder_hidden.cuda(), decoder_cell_state.cuda())\n",
    "        else:\n",
    "            return (decoder_hidden, decoder_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size = n_input_encoder, hidden_size = n_hidden_encoder, output_size = n_hidden_encoder, \n",
    "                 n_layers = 1, dropout_p = 0):\n",
    "        '''\n",
    "        input_size: size of input driving sequence x_t\n",
    "        hidden_size: size of h_t=128\n",
    "        output_size: size of h_t=128 ?\n",
    "        n_layers: number of hidden layers=1\n",
    "        '''\n",
    "        super(AttnEncoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.encoder_input_size = input_size\n",
    "        self.encoder_hidden_size = hidden_size # size of h_t\n",
    "        self.encoder_output_size = output_size # size of h_t\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        #self.dropout = nn.Dropout(dropout_p)\n",
    "        self.encoder_cell = nn.LSTMCell(self.encoder_input_size, self.encoder_hidden_size)\n",
    "        self.attn = Attn(self.encoder_hidden_size, n_steps_encoder)\n",
    "        #print (len(list(self.parameters())))\n",
    "        #print (len(list(self.attn.parameters())))\n",
    "        #print (len(list(self.parameters())))\n",
    "        \n",
    "    def forward(self, encoder_input, encoder_hidden, encoder_cell_state, network_input):\n",
    "        '''\n",
    "        encoder_input(batch_size, input_size): x_t (batch_size x 81)\n",
    "        encoder_hidden(batch_size, hidden_size): initial hidden state\n",
    "        network_input(batch_size, input_size, T): X\n",
    "        '''\n",
    "        #replace encoder_hidden by self.encoder_hidden_state?\n",
    "        attn_weights = self.attn(encoder_hidden, encoder_cell_state, network_input) # B x N\n",
    "        encoder_input_weighted = attn_weights * encoder_input # B x N\n",
    "        \n",
    "        encoder_hidden, encoder_cell_state = self.encoder_cell(encoder_input_weighted, (encoder_hidden, encoder_cell_state))\n",
    "        \n",
    "        return encoder_hidden, encoder_cell_state, encoder_input_weighted\n",
    "    def initHidden(self, batch_size):\n",
    "        encoder_hidden = Variable(torch.zeros(batch_size, self.encoder_hidden_size))\n",
    "        encoder_cell_state = Variable(torch.zeros(batch_size, self.encoder_hidden_size))\n",
    "        if use_cuda:\n",
    "            return (encoder_hidden.cuda(), encoder_cell_state.cuda())\n",
    "        else:\n",
    "            return (encoder_hidden, encoder_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.tar'):\n",
    "    torch.save(state, model_path + filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_path + filename, model_path + 'model_best.tar')\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(input_dict, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    global n_hidden_encoder\n",
    "    batch_sz = input_dict['encoder_attention_states'].shape[0]\n",
    "    encoder_hidden, encoder_cell_state = encoder.initHidden(batch_sz)\n",
    "    encoder_attention_states = Variable(torch.from_numpy(input_dict['encoder_attention_states']).type(dtype)) #numpy\n",
    "    encoder_attn_size = input_dict['encoder_attention_states'].shape[2]\n",
    "    encoder_attn_len = input_dict['encoder_attention_states'].shape[1]\n",
    "    decoder_attn_size = input_dict['decoder_input'].shape[1]\n",
    "    encoder_input = Variable(torch.from_numpy(input_dict['encoder_input']).permute(1,0,2).type(dtype))\n",
    "    encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "    decoder_input = Variable(torch.from_numpy(input_dict['decoder_input']).permute(1,0,2).type(dtype))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_gt = Variable(torch.from_numpy(input_dict['decoder_gt']).type(dtype))\n",
    "    decoder_gt = decoder_gt.cuda() if use_cuda else decoder_gt\n",
    "\n",
    "    encoder_hiddens = Variable(torch.zeros(encoder_attn_size, batch_sz, n_hidden_encoder).type(dtype))\n",
    "    encoder_hiddens = encoder_hiddens.cuda() if use_cuda else encoder_hiddens\n",
    "    encoder_attn_weights = Variable(torch.zeros(encoder_attn_size, batch_sz,encoder_attn_len).type(dtype))\n",
    "    encoder_attn_weights = encoder_attn_weights.cuda() if use_cuda else encoder_attn_weights\n",
    "    decoder_outputs = Variable(torch.zeros(decoder_attn_size, batch_sz, n_hidden_decoder).type(dtype))\n",
    "    decoder_outputs = decoder_outputs.cuda() if use_cuda else decoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(encoder_attn_size):\n",
    "        encoder_hidden, encoder_cell_state, encoder_attention = encoder(\n",
    "            encoder_input[ei], encoder_hidden, encoder_cell_state, encoder_attention_states)\n",
    "        encoder_hiddens[ei] = encoder_hidden\n",
    "        encoder_attn_weights[ei] = encoder_attention\n",
    "\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden, decoder_cell_state = decoder.initHidden(encoder_hiddens[-1], encoder_cell_state) # Will it make a new copy or is it just a pointer?\n",
    "    context_vector = torch.FloatTensor(batch_sz, n_hidden_encoder)\n",
    "\n",
    "    # permute the encoder_ouputs [time_steps * batch_size * hidden_size] \n",
    "    # to [batch_size * time_steps * hidden_size]\n",
    "    encoder_hiddens = encoder_hiddens.permute(1,0,2)\n",
    "    for di in range(decoder_attn_size):\n",
    "        decoder_hidden, decoder_cell_state, decoder_output, context_vector = decoder(\n",
    "            decoder_input[di], decoder_hidden, decoder_cell_state, encoder_hiddens, context_vector)\n",
    "        decoder_outputs[di,:,:] = decoder_output.data\n",
    "    \n",
    "    y_pred = decoder_outputs[-1].mm(input_dict['output_weight']) + (input_dict['output_bias'])\n",
    "    #print (y_pred[0][0])\n",
    "    loss = criterion(y_pred, decoder_gt)\n",
    "    encoder_data = {'encoder_hiddens': encoder_hiddens, 'encoder_cell_state': encoder_cell_state, 'encoder_attn_weights': encoder_attn_weights};\n",
    "    decoder_data = {'decoder_outputs': decoder_outputs, 'decoder_cell_state': decoder_cell_state, 'context_vector': context_vector}\n",
    "    return loss, encoder_data, decoder_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_dict, encoder, decoder, encoder_optimizer, decoder_optimizer, output_optimizer, criterion):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    output_optimizer.zero_grad()\n",
    "    \n",
    "    loss, encoder_data, decoder_data = feedforward(input_dict, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    output_optimizer.step()\n",
    "\n",
    "    return loss.data[0], encoder_data, decoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100):\n",
    "    global learning_rate, n_classes, batch_size, n_steps_encoder, n_steps_decoder, n_hidden_encoder\n",
    "    start = time.time()\n",
    "\n",
    "    output_weight = Variable(torch.randn(n_hidden_decoder, n_classes).type(dtype), requires_grad=True)\n",
    "    output_weight = output_weight.cuda() if use_cuda else output_weight\n",
    "    output_bias = Variable(torch.randn(n_classes).type(dtype), requires_grad=True)\n",
    "    output_bias = output_bias.cuda() if use_cuda else output_bias\n",
    "    \n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=learning_rate)\n",
    "    output_optimizer = optim.RMSprop([output_weight, output_bias], lr=learning_rate)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    count = 1\n",
    "\n",
    "    # read the input data\n",
    "    Data = GD.Input_data(batch_size, n_steps_encoder, n_steps_decoder, n_hidden_encoder)\n",
    "    \n",
    "    #print (len(list(encoder.parameters())))\n",
    "    #print (len(list(decoder.parameters())))\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        # the shape of batch_x is (batch_size, n_steps, n_input)\n",
    "        batch_x, batch_y, prev_y, encoder_states = Data.next_batch()\n",
    "        input_dict = {'encoder_input': batch_x, 'decoder_gt': batch_y, 'decoder_input': prev_y,\n",
    "            'encoder_attention_states':encoder_states, 'output_weight': output_weight, 'output_bias': output_bias}\n",
    "\n",
    "        loss, encoder_data, decoder_data = train(input_dict, encoder, decoder, encoder_optimizer, decoder_optimizer, output_optimizer, criterion)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print('Time : %s ' % (timeSince(start, iter / n_iters)))\n",
    "            print(\"Iter \" + str(iter) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss))\n",
    "\n",
    "            #store the value\n",
    "            loss_value.append(loss)\n",
    "            step_value.append(iter)\n",
    "\n",
    "            # validation\n",
    "            val_x, val_y, val_prev_y, encoder_states_val = Data.validation()\n",
    "            val_dict = {'encoder_input': val_x, 'decoder_gt': val_y, 'decoder_input': val_prev_y,\n",
    "                         'encoder_attention_states':encoder_states_val, 'output_weight': output_weight, 'output_bias': output_bias}\n",
    "\n",
    "            loss_val1, _, _  = feedforward(val_dict, encoder, decoder,\n",
    "                    encoder_optimizer, decoder_optimizer, criterion)\n",
    "            loss_val1 = loss_val1.data[0]\n",
    "            loss_val.append(loss_val1)\n",
    "            print(\"validation Accuracy:\", loss_val1)\n",
    "\n",
    "            # testing\n",
    "            test_x, test_y, test_prev_y, encoder_states_test= Data.testing()\n",
    "            test_dict = {'encoder_input': test_x, 'decoder_gt': test_y, 'decoder_input': test_prev_y,\n",
    "                         'encoder_attention_states':encoder_states_test, 'output_weight': output_weight, 'output_bias': output_bias}\n",
    "            loss_test1, _, _ = feedforward(test_dict, encoder, decoder,\n",
    "                    encoder_optimizer, decoder_optimizer, criterion)\n",
    "            loss_test1 = loss_test1.data[0]\n",
    "            loss_test.append(loss_test1)\n",
    "\n",
    "            print(\"Testing Accuracy:\", loss_test1)\n",
    "\n",
    "            #save the parameters\n",
    "            if loss_val1<=min(loss_val):\n",
    "                is_best = True\n",
    "                save_checkpoint({\n",
    "                    'epoch': iter + 1,\n",
    "                    'encoder_data': encoder_data,\n",
    "                    'decoder_data': decoder_data,\n",
    "                    'encoder_optimizer' : encoder_optimizer.state_dict(),\n",
    "                    'decoder_optimizer' : decoder_optimizer.state_dict(),\n",
    "                    'output_optimizer' : output_optimizer.state_dict(),\n",
    "                }, is_best)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        # reduce the learning rate\n",
    "        if count > 10000:\n",
    "            learning_rate *= 0.1\n",
    "            count = 0\n",
    "            is_best = False\n",
    "            save_checkpoint({\n",
    "                'epoch': iter + 1,\n",
    "                'encoder_data': encoder_data,\n",
    "                'decoder_data': decoder_data,\n",
    "                'encoder_optimizer' : encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer' : decoder_optimizer.state_dict(),\n",
    "                'output_optimizer' : output_optimizer.state_dict(),\n",
    "            }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 0m 53s (- 445m 54s) \n",
      "Iter 100, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 1m 46s (- 442m 11s) \n",
      "Iter 200, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 2m 30s (- 416m 24s) \n",
      "Iter 300, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 3m 13s (- 400m 23s) \n",
      "Iter 400, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 4m 2s (- 400m 41s) \n",
      "Iter 500, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 5m 6s (- 420m 11s) \n",
      "Iter 600, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 6m 2s (- 424m 57s) \n",
      "Iter 700, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 6m 47s (- 417m 37s) \n",
      "Iter 800, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 7m 31s (- 410m 31s) \n",
      "Iter 900, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 8m 17s (- 406m 34s) \n",
      "Iter 1000, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 9m 5s (- 404m 20s) \n",
      "Iter 1100, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n",
      "Time : 9m 52s (- 401m 41s) \n",
      "Iter 1200, Minibatch Loss= nan\n",
      "validation Accuracy: nan\n",
      "Testing Accuracy: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e43dafc90cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mattn_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimization Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-dab58cc9b73e>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every)\u001b[0m\n\u001b[1;32m     26\u001b[0m             'encoder_attention_states':encoder_states, 'output_weight': output_weight, 'output_bias': output_bias}\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-047faf8a79f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_dict, encoder, decoder, encoder_optimizer, decoder_optimizer, output_optimizer, criterion)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9e27fd4a4c79>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(input_dict, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_attn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         encoder_hidden, encoder_cell_state, encoder_attention = encoder(\n\u001b[0;32m---> 27\u001b[0;31m             encoder_input[ei], encoder_hidden, encoder_cell_state, encoder_attention_states)\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mencoder_attn_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/prahalarora/anaconda/envs/Ana3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-181519e47bc2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_input, encoder_hidden, encoder_cell_state, network_input)\u001b[0m\n\u001b[1;32m     32\u001b[0m         '''\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#replace encoder_hidden by self.encoder_hidden_state?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_cell_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mencoder_input_weighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mencoder_input\u001b[0m \u001b[0;31m# B x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/prahalarora/anaconda/envs/Ana3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2ee5b6b7175b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_state, cell_state, encoder_outputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Calculate energy for each encoder output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mattn_energies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_energies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/prahalarora/anaconda/envs/Ana3.6/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# auto-wrap tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ByteTensor'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMaskedSelect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "attn_encoder = AttnEncoderRNN()\n",
    "attn_decoder = AttnDecoderRNN()\n",
    "\n",
    "if use_cuda:\n",
    "    attn_encoder = encoder.cuda()\n",
    "    attn_decoder = attn_decoder.cuda()\n",
    "\n",
    "trainIters(attn_encoder, attn_decoder, training_iters, print_every=display_step)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "decoder_state(batch_size, decoder_state_size): d_t\n",
    "cell_state(batch_size, cell_state_size): s_t\n",
    "hidden_state(batch_size, hidden_size): h_t\n",
    "'''\n",
    "batch_size = 128\n",
    "decoder_state_size = 81\n",
    "cell_state_size = 81\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_t=torch.nn.Parameter(torch.FloatTensor(batch_size, decoder_state_size))\n",
    "s_t=torch.nn.Parameter(torch.FloatTensor(batch_size, cell_state_size))\n",
    "h_t=torch.nn.Parameter(torch.FloatTensor(batch_size, n_steps_encoder, hidden_size))\n",
    "act=nn.Tanh()\n",
    "d_t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "combined_state = torch.cat((d_t, s_t),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_output = h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Attn_Decoder_V = nn.Parameter(torch.FloatTensor(hidden_size,1))\n",
    "Attn_Decoder_W = nn.Linear(decoder_state_size + cell_state_size, hidden_size)\n",
    "Attn_Decoder_U = nn.Parameter(torch.FloatTensor(hidden_size, hidden_size))\n",
    "print(Attn_Decoder_V.size())\n",
    "print(Attn_Decoder_U.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Attn_Decoder_W(combined_state).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy = Attn_Decoder_W(combined_state) + encoder_output.mm(Attn_Decoder_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy = energy.mm(Attn_Decoder_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_energies = Variable(torch.zeros(batch_size, n_steps_encoder)) # B x T\n",
    "for d in range(batch_size):\n",
    "    for i in range(n_steps_encoder):\n",
    "        combined_state = torch.cat((d_t[d,:].unsqueeze(0), s_t[d,:].unsqueeze(0)),1)\n",
    "        encoder_output = h_t[d, i, :]\n",
    "#         pdb.set_trace()\n",
    "        energy=act(Attn_Decoder_W(combined_state) + Attn_Decoder_U(encoder_output)).mm(Attn_Decoder_V).squeeze(0)\n",
    "        attn_energies[d,i] = energy\n",
    "#     attn_energies[:,i] = energy.mm().squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(h_t.size())\n",
    "print(F.softmax(attn_energies).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=F.softmax(attn_energies)\n",
    "y=h_t*x.unsqueeze(2).expand_as(h_t)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attn_energies = Variable(torch.zeros(batch_size, n_steps_encoder)) # B x T\n",
    "attn_energies.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=torch.sum(y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=y[:,-1].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cat((y,z),1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.ones(23,10).unsqueeze(2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.unsqueeze(2).expand_as(torch.randn(2,2,4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "ana3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
